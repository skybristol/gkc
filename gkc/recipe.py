"""
Recipe: Wikidata Barrel Recipe Builder and transformation blueprint design.

This module builds **Wikidata Barrel Recipes** - transformation specifications
that map from the Unified Still Schema (canonical data model) to Wikidata's
format (claims, qualifiers, references).

A Barrel Recipe is generated by analyzing Wikidata's **Barrel Schema**
(EntitySchemas in ShEx format) and fetching live property metadata from
Wikidata. The recipe specifies how Still Schema fields map to Wikidata
properties, with appropriate datatype transformations.

Plain meaning: Build transformation blueprints from Wikidata EntitySchemas to
create Barrel Recipes that enable data distribution to Wikidata.
"""

import re
from datetime import datetime
from typing import Any, Optional, Union

from gkc.cooperage import DEFAULT_USER_AGENT, fetch_entity_schema_metadata
from gkc.sparql import SPARQLError, execute_sparql
from gkc.spirit_safe import SpiritSafeValidator


def _get_preferred_language() -> str:
    import gkc

    languages = gkc.get_languages()
    if languages == "all":
        return "en"
    if isinstance(languages, str):
        return languages
    return languages[0] if languages else "en"


class PropertyLedgerEntry:
    """Container for Wikidata property metadata from the Cooperage.

    Part of Wikidata's Barrel Schema - provides property definitions,
    datatypes, labels, descriptions, and aliases fetched from Wikidata.

    Properties have additional metadata (datatype, formatter URL) that do
    not apply to items.
    """

    def __init__(
        self,
        property_id: str,
        data: dict,
        formatter_url: Optional[str] = None,
    ):
        self.property_id = property_id
        self.datatype = data.get("datatype", "unknown")
        self.labels = data.get("labels", {})
        self.descriptions = data.get("descriptions", {})
        self.aliases = data.get("aliases", {})
        # Formatter URL (P1630) for URL extraction patterns
        self.formatter_url = formatter_url

    def get_label(self, language: str = "en") -> str:
        """Get property label in specified language."""
        if language in self.labels:
            return str(self.labels[language].get("value", self.property_id))
        return self.property_id

    def get_description(self, language: str = "en") -> str:
        """Get property description in specified language."""
        if language in self.descriptions:
            return str(self.descriptions[language].get("value", ""))
        return ""

    def get_formatter_url(self) -> Optional[str]:
        """Get formatter URL pattern (P1630).

        Returns:
            Formatter URL pattern for extracting or formatting values,
            or None if not set.
        """
        return self.formatter_url


class ItemLedgerEntry:
    """Container for Wikidata item metadata from the Cooperage.

    Part of Wikidata's Barrel Schema - provides item labels, descriptions,
    and aliases fetched from Wikidata.
    """

    def __init__(self, item_id: str, data: dict):
        self.item_id = item_id
        self.labels = data.get("labels", {})
        self.descriptions = data.get("descriptions", {})
        self.aliases = data.get("aliases", {})

    def get_label(self, language: str = "en") -> str:
        """Get item label in specified language."""
        if language in self.labels:
            return str(self.labels[language].get("value", self.item_id))
        return self.item_id

    def get_description(self, language: str = "en") -> str:
        """Get item description in specified language."""
        if language in self.descriptions:
            return str(self.descriptions[language].get("value", ""))
        return ""


class SpecificationExtractor:
    """Extracts property information from ShExC EntitySchema specifications.

    Parses Wikidata's Barrel Schema (EntitySchemas written in ShEx) to extract:
    - Property IDs and their contexts (statement, qualifier, reference)
    - Cardinality constraints (required, optional, multiple)
    - Value constraints (e.g., P31 instance-of, P279 subclass-of restrictions)
    - Inline documentation from comments

    """

    # Pattern to match property references like wdt:P31, p:P571, etc.
    PROPERTY_PATTERN = re.compile(r"\b(wdt?|p|ps|pq|pr):P(\d+)\b")

    # Pattern to match item references like wd:Q123
    ITEM_PATTERN = re.compile(r"\bwd:Q(\d+)\b")

    # Pattern to match shape definitions with comments
    SHAPE_PATTERN = re.compile(r"<(\w+)>\s*{([^}]+)}", re.MULTILINE | re.DOTALL)

    # Pattern to match inline comments
    COMMENT_PATTERN = re.compile(r"#\s*(.+?)(?:\n|$)")

    # Pattern to match value constraints in brackets [Q123], [Q456|Q789], @<shape>
    VALUE_CONSTRAINT_PATTERN = re.compile(r"\[([^\]]+)\]|@<(\w+)>")

    def __init__(self, schema_text: str):
        self.schema_text = schema_text
        self.properties: dict[str, dict] = {}
        self.shape_comments: dict[str, str] = {}
        self.instance_of_constraints: list[str] = []
        self.subclass_of_constraints: list[str] = []
        self.start_shape_id: Optional[str] = None

    def extract(self) -> dict[str, dict]:
        """
        Extract all properties from ShExC specification with context.

        Returns:
            Dictionary mapping property IDs to their context information
        """
        self._extract_shapes()
        self._extract_properties()
        self._extract_classification_constraints()
        return self.properties

    def extract_items(self) -> list[str]:
        """
        Extract all item IDs (Q-IDs) from ShExC specification.

        Uses PyShEx's structured parser to extract Q-IDs from value constraints
        in the ShEx schema. If the schema cannot be parsed, returns an empty list.
        No text-based fallbacks are used - if PyShEx can't parse it, we don't trust it.

        Returns:
            List of unique item IDs (e.g., ['Q30', 'Q7840353'])
        """
        item_ids = self._extract_items_from_shexj()
        return sorted(item_ids)

    def _extract_items_from_shexj(self) -> set[str]:
        """
        Extract Q-IDs using PyShEx's structured ShExJ parser.

        Returns:
            Set of item IDs found, or empty set if PyShEx parsing fails
        """
        try:
            from pyshex.utils.schema_loader import SchemaLoader
        except Exception:
            return set()

        try:
            loader = SchemaLoader()
            schema = loader.loads(self.schema_text)
        except Exception:
            return set()

        item_ids = set()

        def _extract_qids_from_text(text: str) -> list[str]:
            """Extract Q-IDs from text representation."""
            return re.findall(r"Q\d+", text or "")

        def _value_expr_to_qids(value_expr: Any) -> list[str]:
            """Recursively extract Q-IDs from a value expression."""
            if value_expr is None:
                return []

            qids: list[str] = []

            # NodeConstraint with explicit values
            values = getattr(value_expr, "values", None)
            if values is not None:
                for val in values:
                    qids.extend(_extract_qids_from_text(str(val)))
                return qids

            # ShapeOr/ShapeAnd with nested expressions
            shape_exprs = getattr(value_expr, "shapeExprs", None)
            if shape_exprs is not None:
                for expr in shape_exprs:
                    qids.extend(_value_expr_to_qids(expr))
                return qids

            # ShapeNot wrapper
            inner = getattr(value_expr, "shapeExpr", None)
            if inner is not None:
                qids.extend(_value_expr_to_qids(inner))
                return qids

            # Fallback: try to parse QIDs from string representation
            qids.extend(_extract_qids_from_text(str(value_expr)))
            return qids

        def _walk_triple_constraint(tc: Any) -> list[str]:
            """Extract Q-IDs from a triple constraint."""
            qids: list[str] = []
            value_expr = getattr(tc, "valueExpr", None)
            if value_expr is not None:
                qids.extend(_value_expr_to_qids(value_expr))
            return qids

        def _walk_expression(expr: Any) -> list[str]:
            """Walk a shape expression and extract Q-IDs."""
            if expr is None:
                return []

            qids: list[str] = []

            # TripleConstraint
            if hasattr(expr, "predicate"):
                qids.extend(_walk_triple_constraint(expr))

            # EachOf/OneOf with nested expressions
            expressions = getattr(expr, "expressions", None)
            if expressions is not None:
                for sub_expr in expressions:
                    qids.extend(_walk_expression(sub_expr))

            return qids

        # Walk all shapes in the schema
        for shape in getattr(schema, "shapes", None) or []:
            expr = getattr(shape, "expression", None)
            if expr is not None:
                item_ids.update(_walk_expression(expr))

        return item_ids

    def get_instance_of_constraints(self) -> list[str]:
        """
        Get QIDs restricted by P31 (instance of) constraints in the schema.

        Returns:
            List of QID strings found in P31 value constraints
        """
        if not self.properties and not self.instance_of_constraints:
            self.extract()
        return self._normalize_constraint_values(self.instance_of_constraints)

    def get_subclass_of_constraints(self) -> list[str]:
        """
        Get QIDs restricted by P279 (subclass of) constraints in the schema.

        Returns:
            List of QID strings found in P279 value constraints
        """
        if not self.properties and not self.subclass_of_constraints:
            self.extract()
        return self._normalize_constraint_values(self.subclass_of_constraints)

    def get_start_shape_id(self) -> Optional[str]:
        """
        Get the schema start shape identifier, if available.

        Returns:
            Start shape identifier as a string, or None if unavailable
        """
        if self.start_shape_id is None:
            self._extract_classification_constraints_shexj()
        return self.start_shape_id

    def _extract_shapes(self):
        """Extract shape definitions and their comments."""
        for match in self.SHAPE_PATTERN.finditer(self.schema_text):
            shape_name = match.group(1)
            shape_body = match.group(2)
            self.shape_comments[shape_name] = self._extract_shape_properties(shape_body)

    def _extract_shape_properties(self, shape_body: str) -> dict:
        """Extract properties from a shape body with their comments."""
        properties = {}
        lines = shape_body.split("\n")

        for line in lines:
            # Look for property references
            prop_match = self.PROPERTY_PATTERN.search(line)
            if prop_match:
                prefix = prop_match.group(1)
                prop_num = prop_match.group(2)
                prop_id = f"P{prop_num}"

                # Extract comment if present
                comment_match = self.COMMENT_PATTERN.search(line)
                comment = comment_match.group(1).strip() if comment_match else ""

                # Determine cardinality
                cardinality = self._extract_cardinality(line)

                # Determine if it's a qualifier or reference
                context = self._determine_context(prefix)

                # Extract value constraints
                constraints = self._extract_value_constraints(line)

                if prop_id not in properties:
                    properties[prop_id] = {
                        "property_id": prop_id,
                        "comment": comment,
                        "cardinality": cardinality,
                        "context": context,
                        "prefix": prefix,
                        "full_line": line.strip(),
                        "constraints": constraints,
                    }

        return properties

    def _extract_value_constraints(self, line: str) -> list[str]:
        """Extract value constraints from a property line.

        Handles patterns like:
        - [Q123] - single value
        - [Q456|Q789] - multiple values
        - @<shape> - reference to shape
        """
        constraints = []
        for match in self.VALUE_CONSTRAINT_PATTERN.finditer(line):
            if match.group(1):  # Bracketed values
                values_str = match.group(1)
                # Split on | for multiple values
                for val in values_str.split("|"):
                    val = val.strip()
                    if val:
                        constraints.append(val)
            elif match.group(2):  # Shape reference
                constraints.append(f"@<{match.group(2)}>")
        return constraints

    def _extract_classification_constraints(self):
        """Extract P31 (instance of) and P279 (subclass of) value constraints."""
        if self._extract_classification_constraints_shexj():
            return

        # Fallback: Check all extracted properties (regex-based)
        for prop_id, prop_info in self.properties.items():
            constraints = prop_info.get("constraints", [])

            if prop_id == "P31":
                # P31 = instance of
                self.instance_of_constraints.extend(constraints)
            elif prop_id == "P279":
                # P279 = subclass of
                self.subclass_of_constraints.extend(constraints)

    def _extract_classification_constraints_shexj(self) -> bool:
        """Extract classification constraints using PyShEx's ShExJ parser.

        Returns:
            True if ShExJ parsing succeeded, False otherwise
        """
        try:
            from pyshex.utils.schema_loader import SchemaLoader
            from ShExJSG import ShExJ
        except Exception:
            return False

        try:
            loader = SchemaLoader()
            schema = loader.loads(self.schema_text)
        except Exception:
            return False

        def _extract_qids_from_text(text: str) -> list[str]:
            return re.findall(r"Q\d+", text or "")

        def _value_expr_to_qids(value_expr: Any) -> list[str]:
            if value_expr is None:
                return []

            qids: list[str] = []

            # NodeConstraint with explicit values
            values = getattr(value_expr, "values", None)
            if values is not None:
                for val in values:
                    qids.extend(_extract_qids_from_text(str(val)))
                return qids

            # ShapeOr/ShapeAnd with nested expressions
            shape_exprs = getattr(value_expr, "shapeExprs", None)
            if shape_exprs is not None:
                for expr in shape_exprs:
                    qids.extend(_value_expr_to_qids(expr))
                return qids

            # ShapeNot wrapper
            inner = getattr(value_expr, "shapeExpr", None)
            if inner is not None:
                qids.extend(_value_expr_to_qids(inner))
                return qids

            # ShapeRef or other reference-like nodes
            ref = getattr(value_expr, "reference", None)
            if ref is None:
                ref = getattr(value_expr, "id", None)
            if ref is None:
                ref = getattr(value_expr, "label", None)

            if ref is not None:
                shape = _resolve_shape_ref(ref)
                if shape is not None:
                    qids.extend(_walk_shape(shape))
                    return qids

            if isinstance(value_expr, str):
                shape = _resolve_shape_ref(value_expr)
                if shape is not None:
                    qids.extend(_walk_shape(shape))
                    return qids

            # Fallback: try to parse QIDs from string representation
            qids.extend(_extract_qids_from_text(str(value_expr)))
            return qids

        def _predicate_to_pid(predicate: Any) -> Optional[str]:
            text = str(predicate)
            if re.search(r"P31\b", text):
                return "P31"
            if re.search(r"P279\b", text):
                return "P279"
            return None

        shape_map: dict[str, Any] = {}
        for shape in getattr(schema, "shapes", None) or []:
            shape_id = getattr(shape, "id", None)
            if shape_id is not None:
                shape_map[str(shape_id)] = shape

        visited_shapes: set[str] = set()

        def _resolve_shape_ref(ref: Any) -> Optional[Any]:
            ref_text = str(ref)
            if ref_text in shape_map:
                return shape_map[ref_text]
            # Try to match by local identifier (e.g., <ShapeName>)
            local_match = re.search(r"<?(\w+)>?", ref_text)
            if local_match:
                local = local_match.group(1)
                for key, shape in shape_map.items():
                    if (
                        key.endswith(local)
                        or key.endswith(f"/{local}")
                        or key.endswith(f"#{local}")
                    ):
                        return shape
            return None

        def _local_shape_name(shape_id: Any) -> str:
            if shape_id is None:
                return ""
            text = str(shape_id)
            # Prefer trailing fragment or path segment
            for sep in ("#", "/"):
                if sep in text:
                    text = text.split(sep)[-1]
            local_match = re.search(r"(\w+)$", text)
            return local_match.group(1) if local_match else text

        def _walk_shape(shape: Any) -> list[str]:
            shape_id = getattr(shape, "id", None)
            shape_id_text = str(shape_id) if shape_id is not None else None
            if shape_id_text and shape_id_text in visited_shapes:
                return []
            if shape_id_text:
                visited_shapes.add(shape_id_text)

            expr = getattr(shape, "expression", None)
            if expr is None:
                expr = getattr(shape, "shapeExpr", None)
            _walk_expression(expr)
            return []

        def _walk_expression(expr: Any):
            if expr is None:
                return

            # TripleConstraint
            if isinstance(expr, ShExJ.TripleConstraint):
                prop_id = _predicate_to_pid(expr.predicate)
                if prop_id:
                    qids = _value_expr_to_qids(expr.valueExpr)
                    if prop_id == "P31":
                        self.instance_of_constraints.extend(qids)
                    elif prop_id == "P279":
                        self.subclass_of_constraints.extend(qids)
                return

            # ShapeRef (duck-typed)
            ref = getattr(expr, "reference", None)
            if ref is not None:
                shape = _resolve_shape_ref(ref)
                if shape is not None:
                    _walk_shape(shape)
                return

            # EachOf / OneOf
            expressions = getattr(expr, "expressions", None)
            if expressions is not None:
                for child in expressions:
                    _walk_expression(child)
                return

            # Shape (contains .expression)
            inner = getattr(expr, "expression", None)
            if inner is not None:
                _walk_expression(inner)
                return

        shapes = getattr(schema, "shapes", None) or []
        start = getattr(schema, "start", None)

        if start is not None:
            start_shape = _resolve_shape_ref(start)
            if start_shape is not None:
                self.start_shape_id = _local_shape_name(
                    getattr(start_shape, "id", start)
                )
                _walk_shape(start_shape)
            else:
                for shape in shapes:
                    _walk_shape(shape)
        else:
            for shape in shapes:
                _walk_shape(shape)

        return True

    def _normalize_constraint_values(self, constraints: list[str]) -> list[str]:
        """Normalize and deduplicate constraint values.

        Removes shape references (@<shape>) and extracts QIDs.
        """
        qids = []
        for constraint in constraints:
            # Skip shape references
            if constraint.startswith("@<"):
                continue
            # Extract QIDs from values like "Q123", "wd:Q123", or full URIs
            for qid in re.findall(r"Q\d+", constraint):
                qids.append(qid)
        # Return deduplicated, sorted list
        return sorted(list(set(qids)))

    def _extract_properties(self):
        """Combine all property information from shapes."""
        for shape_name, shape_props in self.shape_comments.items():
            for prop_id, prop_info in shape_props.items():
                if prop_id not in self.properties:
                    self.properties[prop_id] = prop_info
                    self.properties[prop_id]["shapes"] = [shape_name]
                else:
                    # Property appears in multiple shapes
                    if "shapes" not in self.properties[prop_id]:
                        self.properties[prop_id]["shapes"] = []
                    self.properties[prop_id]["shapes"].append(shape_name)

    def _extract_cardinality(self, line: str) -> dict[str, Any]:
        """Extract cardinality information from a property line."""
        cardinality: dict[str, Any] = {"min": 1, "max": 1}  # Default: exactly one

        # Check for cardinality indicators
        if line.strip().endswith(";"):
            # Exactly one (required)
            cardinality = {"min": 1, "max": 1, "required": True}
        elif line.strip().endswith("?"):
            # Zero or one (optional)
            cardinality = {"min": 0, "max": 1, "required": False}
        elif line.strip().endswith("*"):
            # Zero or more
            cardinality = {"min": 0, "max": None, "required": False}
        elif line.strip().endswith("+"):
            # One or more
            cardinality = {"min": 1, "max": None, "required": True}

        return cardinality

    def _determine_context(self, prefix: str) -> str:
        """Determine the context of a property based on its prefix."""
        context_map = {
            "wdt": "direct",  # Direct property value
            "wd": "item",  # Item reference
            "p": "statement",  # Full statement
            "ps": "statement_value",  # Statement value
            "pq": "qualifier",  # Qualifier
            "pr": "reference",  # Reference
        }
        return context_map.get(prefix, "unknown")


class EntityCatalog:
    """Fetches and caches entity metadata from Wikidata using SPARQL.

    Part of the Cooperage - manages access to Wikidata's Barrel Schema
    by fetching property and item definitions, datatypes, labels, descriptions,
    and aliases.

    Uses SPARQL queries for efficient batch retrieval of entities, replacing
    multiple REST API calls with a single query.
    """

    def __init__(
        self,
        user_agent: Optional[str] = None,
        max_entities: int = 500,
        fetch_property_details: bool = False,
    ):
        """
        Initialize the entity catalog.

        Args:
            user_agent: Custom user agent for API requests
            max_entities: Maximum number of entities to fetch (default 500)
            fetch_property_details: Whether to fetch property-only metadata
                like datatype and formatter URL (default False).
        """
        self.user_agent = user_agent or DEFAULT_USER_AGENT
        self.max_entities = max_entities
        self.fetch_property_details = fetch_property_details
        self._property_cache: dict[str, PropertyLedgerEntry] = {}
        self._item_cache: dict[str, ItemLedgerEntry] = {}

    def _get_language_codes(self) -> list[str]:
        import gkc

        languages = gkc.get_languages()
        if languages == "all":
            return []
        if isinstance(languages, str):
            return [languages]
        if "all" in languages:
            return []
        return [lang for lang in languages if lang]

    @staticmethod
    def _build_language_filter(lang_var: str, language_codes: list[str]) -> str:
        if not language_codes:
            return ""
        language_list = ", ".join([f'"{lang}"' for lang in language_codes])
        return f"FILTER({lang_var} IN ({language_list}))"

    def fetch_entities(
        self, entity_ids: list[str]
    ) -> dict[str, Union[PropertyLedgerEntry, ItemLedgerEntry]]:
        """Fetch metadata for properties and items using a single SPARQL query.

        Args:
            entity_ids: List of entity IDs (e.g., ['P31', 'Q5'])

        Returns:
            Dictionary mapping entity IDs to PropertyLedgerEntry or
            ItemLedgerEntry objects.

        Raises:
            ValueError: If entity_ids exceeds max_entities threshold
        """
        if not entity_ids:
            return {}

        if len(entity_ids) > self.max_entities:
            raise ValueError(
                f"Cannot fetch {len(entity_ids)} entities. "
                f"Maximum allowed: {self.max_entities}"
            )

        prop_ids = [eid for eid in entity_ids if eid.startswith("P")]
        item_ids = [eid for eid in entity_ids if eid.startswith("Q")]

        uncached_ids = [eid for eid in prop_ids if eid not in self._property_cache] + [
            eid for eid in item_ids if eid not in self._item_cache
        ]

        if uncached_ids:
            self._fetch_via_sparql(uncached_ids)

        results: dict[str, Union[PropertyLedgerEntry, ItemLedgerEntry]] = {}
        for entity_id in entity_ids:
            if entity_id in self._property_cache:
                results[entity_id] = self._property_cache[entity_id]
            elif entity_id in self._item_cache:
                results[entity_id] = self._item_cache[entity_id]

        return results

    def _fetch_via_sparql(self, entity_ids: list[str]) -> None:
        """Fetch entity metadata via SPARQL query.

        Args:
            entity_ids: List of entity IDs (e.g., ['P31', 'Q5'])
        """
        try:
            entity_iris = [f"wd:{entity_id}" for entity_id in entity_ids]
            query = self._build_entity_query(entity_iris)
            results = execute_sparql(query)
            if "results" in results and "bindings" in results["results"]:
                self._parse_sparql_results(results["results"]["bindings"])
        except SPARQLError as e:
            print(f"Warning: SPARQL query failed: {e}")
        except Exception as e:
            print(f"Warning: Failed to fetch via SPARQL: {e}")

    def _build_entity_query(self, entity_iris: list[str]) -> str:
        """Build a SPARQL query for fetching entity metadata.

        Args:
            entity_iris: List of entity IRIs

        Returns:
            SPARQL query string
        """
        entity_list = " ".join(entity_iris)
        language_codes = self._get_language_codes()

        label_filter = self._build_language_filter("?labelLang", language_codes)
        alias_filter = self._build_language_filter("?aliasLang", language_codes)
        desc_filter = self._build_language_filter("?descLang", language_codes)

        select_vars = "?entityId ?label ?labelLang ?alias ?aliasLang ?desc ?descLang"
        property_blocks = ""
        if self.fetch_property_details:
            select_vars += " ?datatype ?formatterUrl"
            property_blocks = """
            OPTIONAL { ?entityId wikibase:propertyType ?datatype . }
            OPTIONAL { ?entityId wdt:P1630 ?formatterUrl . }
            """

        label_block = f"""
            OPTIONAL {{
                ?entityId rdfs:label ?label .
                BIND(LANG(?label) AS ?labelLang)
                {label_filter}
            }}
        """

        alias_block = f"""
            OPTIONAL {{
                ?entityId skos:altLabel ?alias .
                BIND(LANG(?alias) AS ?aliasLang)
                {alias_filter}
            }}
        """

        desc_block = f"""
            OPTIONAL {{
                ?entityId schema:description ?desc .
                BIND(LANG(?desc) AS ?descLang)
                {desc_filter}
            }}
        """

        query = f"""
        PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>
        PREFIX skos: <http://www.w3.org/2004/02/skos/core#>
        PREFIX schema: <http://schema.org/>
        PREFIX wikibase: <http://wikiba.se/ontology#>
        PREFIX wdt: <http://www.wikidata.org/prop/direct/>

        SELECT DISTINCT {select_vars}
        WHERE {{
            VALUES ?entityId {{ {entity_list} }}
            {label_block}
            {alias_block}
            {desc_block}
            {property_blocks}
        }}
        """

        return query

    def _parse_sparql_results(self, bindings: list[dict]) -> None:
        """Parse SPARQL results and populate caches.

        Args:
            bindings: SPARQL results bindings
        """
        entities_data: dict[str, dict[str, Any]] = {}

        for binding in bindings:
            entity_iri = binding.get("entityId", {}).get("value", "")
            entity_id = entity_iri.split("/")[-1] if entity_iri else None

            if not entity_id:
                continue

            if entity_id not in entities_data:
                entities_data[entity_id] = {
                    "labels": {},
                    "descriptions": {},
                    "aliases": {},
                    "datatype": None,
                    "formatter_url": None,
                }

            label_value = binding.get("label", {}).get("value")
            label_lang = binding.get("labelLang", {}).get("value")
            if label_value and label_lang:
                entities_data[entity_id]["labels"].setdefault(label_lang, label_value)

            desc_value = binding.get("desc", {}).get("value")
            desc_lang = binding.get("descLang", {}).get("value")
            if desc_value and desc_lang:
                entities_data[entity_id]["descriptions"].setdefault(
                    desc_lang, desc_value
                )

            alias_value = binding.get("alias", {}).get("value")
            alias_lang = binding.get("aliasLang", {}).get("value")
            if alias_value and alias_lang:
                aliases = entities_data[entity_id]["aliases"].setdefault(
                    alias_lang, set()
                )
                aliases.add(alias_value)

            if entity_id.startswith("P"):
                datatype_value = binding.get("datatype", {}).get("value")
                if datatype_value:
                    entities_data[entity_id]["datatype"] = datatype_value

                formatter_value = binding.get("formatterUrl", {}).get("value")
                if formatter_value:
                    entities_data[entity_id]["formatter_url"] = formatter_value

        for entity_id, data in entities_data.items():
            labels = {lang: {"value": value} for lang, value in data["labels"].items()}
            descriptions = {
                lang: {"value": value} for lang, value in data["descriptions"].items()
            }
            aliases = {
                lang: [{"value": value} for value in sorted(values)]
                for lang, values in data["aliases"].items()
            }

            if entity_id.startswith("P"):
                ledger_data = {
                    "labels": labels,
                    "descriptions": descriptions,
                    "aliases": aliases,
                    "datatype": data["datatype"],
                }
                self._property_cache[entity_id] = PropertyLedgerEntry(
                    entity_id,
                    ledger_data,
                    formatter_url=data["formatter_url"],
                )
            else:
                ledger_data = {
                    "labels": labels,
                    "descriptions": descriptions,
                    "aliases": aliases,
                }
                self._item_cache[entity_id] = ItemLedgerEntry(entity_id, ledger_data)


class RecipeBuilder:
    """
    Wikidata Barrel Recipe Builder - generates transformation recipes from
    EntitySchemas.

    Builds **Wikidata Barrel Recipes** by combining:
    1. Wikidata's Barrel Schema (EntitySchemas in ShEx)
    2. Live property metadata from Wikidata API (via Cooperage)
    3. Transform hints based on Wikidata datatypes

    The generated Barrel Recipe maps from Unified Still Schema fields to
    Wikidata claims, enabling data distribution from the canonical Still Schema
    to Wikidata's specific format.

    Plain meaning: Design Wikidata transformation blueprints from EntitySchemas.
    """

    def __init__(
        self,
        eid: Optional[str] = None,
        schema_text: Optional[str] = None,
        schema_file: Optional[str] = None,
        user_agent: Optional[str] = None,
        max_entities: int = 500,
    ):
        """
        Initialize the recipe builder.

        Args:
            eid: EntitySchema ID (e.g., 'E502')
            schema_text: ShExC schema as text
            schema_file: Path to ShExC schema file
            user_agent: Custom user agent for API requests
            max_entities: Maximum entities to fetch from Wikidata (default 500)
        """
        self.validator = SpiritSafeValidator(
            eid=eid, schema_text=schema_text, schema_file=schema_file
        )
        self.user_agent = user_agent
        self.schema_text: Optional[str] = None
        self.max_entities = max_entities
        self.entity_catalog = EntityCatalog(
            user_agent=user_agent,
            max_entities=max_entities,
            fetch_property_details=True,
        )

        # Ledgers for entity metadata
        self.property_ledger: dict[str, PropertyLedgerEntry] = {}
        self.item_ledger: dict[str, ItemLedgerEntry] = {}

    def load_specification(self) -> "RecipeBuilder":
        """Load the ShExC specification."""
        self.validator.load_specification()
        self.schema_text = self.validator._schema
        return self

    def fetch_entity_metadata(self) -> "RecipeBuilder":
        """
        Extract and fetch metadata for all properties and items in the schema.

        Populates property_ledger and item_ledger with entity metadata
        from Wikidata API. Enforces max_entities limit to prevent runaway API calls.

        Respects RDF structure by extracting only entities referenced with proper
        prefixes (wdt:/p:/ps:/pq:/pr: for properties, wd: for items), excluding
        any IDs that appear only in comments.

        Returns:
            Self for method chaining

        Raises:
            ValueError: If schema contains more entities than max_entities threshold
        """
        if not self.schema_text:
            self.load_specification()

        if not self.schema_text:
            raise ValueError("Schema text could not be loaded")

        # Extract all property IDs (P-IDs) using RDF-aware extraction
        extractor = SpecificationExtractor(self.schema_text)
        shex_properties = extractor.extract()
        property_ids = list(shex_properties.keys())

        # Extract all item IDs (Q-IDs) using RDF-aware extraction
        item_ids = extractor.extract_items()

        if property_ids or item_ids:
            entity_ids = property_ids + item_ids
            entries = self.entity_catalog.fetch_entities(entity_ids)
            self.property_ledger = {
                entity_id: entry
                for entity_id, entry in entries.items()
                if isinstance(entry, PropertyLedgerEntry)
            }
            self.item_ledger = {
                entity_id: entry
                for entity_id, entry in entries.items()
                if isinstance(entry, ItemLedgerEntry)
            }

        return self

    def assemble_recipe(
        self, include_qualifiers: bool = True, include_references: bool = True
    ) -> list[dict[str, Any]]:
        """
        Assemble a Wikidata Barrel Recipe (claims mapping) from EntitySchema.

        Generates transformation specifications that map Still Schema fields
        to Wikidata properties, with datatype-appropriate transforms.

        Args:
            include_qualifiers: Whether to include qualifier properties
            include_references: Whether to include reference properties

        Returns:
            List of claim mapping dictionaries (Barrel Recipe components)
        """
        if not self.schema_text:
            self.load_specification()

        if not self.schema_text:
            raise ValueError("Schema text could not be loaded")

        # Extract properties from ShExC
        extractor = SpecificationExtractor(self.schema_text)
        shex_properties = extractor.extract()

        # Separate by context
        statement_props = {
            pid: info
            for pid, info in shex_properties.items()
            if info["context"] in ["direct", "statement", "statement_value"]
        }
        qualifier_props = {
            pid: info
            for pid, info in shex_properties.items()
            if info["context"] == "qualifier"
        }
        reference_props = {
            pid: info
            for pid, info in shex_properties.items()
            if info["context"] == "reference"
        }

        # Fetch property metadata from Wikidata
        all_prop_ids = list(shex_properties.keys())
        property_entries = self.entity_catalog.fetch_entities(all_prop_ids)
        property_info = {
            entity_id: entry
            for entity_id, entry in property_entries.items()
            if isinstance(entry, PropertyLedgerEntry)
        }

        # Build claims mapping
        claims_map = []

        for prop_id, shex_info in statement_props.items():
            prop_data = property_info.get(prop_id)

            claim_mapping = {
                "property": prop_id,
                "comment": self._format_comment(shex_info, prop_data),
                "source_field": f"{prop_id.lower()}_value",
                "datatype": prop_data.datatype if prop_data else "unknown",
                "required": shex_info["cardinality"].get("required", False),
            }

            # Add transform hints based on datatype
            if prop_data:
                transform_hint = self._get_transform_hint(prop_data.datatype)
                if transform_hint:
                    claim_mapping["transform"] = transform_hint

            # Add empty references and qualifiers arrays if available in schema
            # Users will customize these for their specific source data
            if include_references and reference_props:
                claim_mapping["references"] = []

            if include_qualifiers and qualifier_props:
                claim_mapping["qualifiers"] = []

            claims_map.append(claim_mapping)

        return claims_map

    def _build_reference_library(self) -> dict[str, Any]:
        """
        Build reference library from schema reference properties.

        Returns:
            Dictionary with reference configurations keyed by meaningful names
        """
        if not self.schema_text:
            self.load_specification()

        if not self.schema_text:
            raise ValueError("Schema text could not be loaded")

        extractor = SpecificationExtractor(self.schema_text)
        shex_properties = extractor.extract()

        # Find all reference properties
        reference_props = {
            pid: info
            for pid, info in shex_properties.items()
            if info["context"] == "reference"
        }

        if not reference_props:
            return {}

        # Fetch property metadata from Wikidata
        property_entries = self.entity_catalog.fetch_entities(
            list(reference_props.keys())
        )
        property_info = {
            entity_id: entry
            for entity_id, entry in property_entries.items()
            if isinstance(entry, PropertyLedgerEntry)
        }

        # Build reference configurations keyed by meaningful names
        references = {}
        for prop_id, shex_info in reference_props.items():
            prop_data = property_info.get(prop_id)

            # Generate unique config name
            config_name = self._generate_config_name(
                prop_id, shex_info.get("comment", ""), shex_info.get("shapes", [])
            )

            references[config_name] = {
                "property": prop_id,
                "datatype": prop_data.datatype if prop_data else "unknown",
                "required": shex_info["cardinality"].get("min", 0) > 0,
                "description": shex_info.get("comment", ""),
            }

        return references

    def _build_qualifier_library(self) -> dict[str, Any]:
        """
        Build qualifier library from schema qualifier properties.

        Returns:
            Dictionary with qualifier configurations keyed by meaningful names
        """
        if not self.schema_text:
            self.load_specification()

        if not self.schema_text:
            raise ValueError("Schema text could not be loaded")

        extractor = SpecificationExtractor(self.schema_text)
        shex_properties = extractor.extract()

        # Find all qualifier properties
        qualifier_props = {
            pid: info
            for pid, info in shex_properties.items()
            if info["context"] == "qualifier"
        }

        if not qualifier_props:
            return {}

        # Fetch property metadata from Wikidata
        property_entries = self.entity_catalog.fetch_entities(
            list(qualifier_props.keys())
        )
        property_info = {
            entity_id: entry
            for entity_id, entry in property_entries.items()
            if isinstance(entry, PropertyLedgerEntry)
        }

        # Build qualifier configurations keyed by meaningful names
        qualifiers = {}
        used_names: set[str] = set()

        for prop_id, shex_info in qualifier_props.items():
            prop_data = property_info.get(prop_id)

            # Generate unique config name
            config_name = self._generate_config_name(
                prop_id, shex_info.get("comment", ""), shex_info.get("shapes", [])
            )

            # Handle name collisions
            if config_name in used_names:
                config_name = f"{config_name}_{prop_id.lower()}"
            used_names.add(config_name)

            qualifiers[config_name] = {
                "property": prop_id,
                "datatype": prop_data.datatype if prop_data else "unknown",
                "required": shex_info["cardinality"].get("min", 0) > 0,
                "description": shex_info.get("comment", ""),
            }

        return qualifiers

    @staticmethod
    def _generate_config_name(
        prop_id: str, comment: str, shape_names: list[str]
    ) -> str:
        """
        Generate a unique configuration name from property, comment, and shapes.

        Args:
            prop_id: Property ID (e.g., "P248")
            comment: Inline comment/description from ShEx
            shape_names: List of shape names used for this property

        Returns:
            A readable, unique configuration name
        """
        import re

        # Extract meaningful words from comment
        comment_words = comment.lower().replace(":", "").split()
        stop_words = {
            "of",
            "or",
            "in",
            "at",
            "the",
            "on",
            "a",
            "an",
            "with",
            "must",
            "have",
            "least",
            "one",
            "required",
        }
        meaningful = [w for w in comment_words if w not in stop_words and len(w) > 2]

        if meaningful:
            # Take first few meaningful words
            name_suffix = "_".join(meaningful[:2])
            name_suffix = re.sub(r"[^a-z0-9_]", "", name_suffix)
            return f"{prop_id.lower()}_{name_suffix}"

        # Fall back to property ID + first shape part if available
        if shape_names and shape_names[0]:
            shape = shape_names[0]
            shape_part = re.sub(r"(?<!^)(?=[A-Z])", "_", shape).lower()
            return f"{prop_id.lower()}_{shape_part}"

        return prop_id.lower()

    def finalize_recipe(self, entity_type: Optional[str] = None) -> dict[str, Any]:
        """
        Finalize a complete Wikidata Barrel Recipe configuration.

        Generates a full transformation specification including labels, aliases,
        descriptions, claims, qualifiers, and references. The recipe defines
        how to transform data from the Unified Still Schema to Wikidata format.

        Args:
            entity_type: Wikidata QID(s) of the entity type(s). Can be a single QID
                string, list of QIDs, or None to auto-detect from schema constraints
                (P31/P279). Examples: 'Q7840353', ['Q123', 'Q456'], None

        Returns:
            Complete Wikidata Barrel Recipe (transformation configuration)
        """
        if not self.schema_text:
            self.load_specification()

        claims_map = self.assemble_recipe()

        # Fetch EntitySchema metadata for enhanced recipe naming
        schema_metadata = {}
        if self.validator.eid:
            try:
                schema_metadata = fetch_entity_schema_metadata(
                    self.validator.eid, user_agent=self.user_agent
                )
            except Exception:
                # If metadata fetch fails, continue with basic info
                schema_metadata = {
                    "label": "",
                    "description": "",
                    "aliases": [],
                    "source": f"https://www.wikidata.org/wiki/EntitySchema:{self.validator.eid}",
                }

        # Extract classification constraints from schema (P31 and P279)
        if not self.schema_text:
            raise ValueError("Schema text could not be loaded")

        extractor = SpecificationExtractor(self.schema_text)
        instance_of_qids = extractor.get_instance_of_constraints()
        subclass_of_qids = extractor.get_subclass_of_constraints()
        start_shape_id = extractor.get_start_shape_id()

        # Resolve entity_type: explicit parameter, schema constraints, or None
        entity_types = self._resolve_entity_types(
            entity_type, instance_of_qids, subclass_of_qids
        )

        # Build recipe name and description from EntitySchema metadata
        schema_label = schema_metadata.get("label", "")
        schema_description = schema_metadata.get("description", "")
        schema_source = schema_metadata.get("source", "")

        if schema_label:
            recipe_name = (
                f"GKC Barrel Recipe derived from Wikidata Entity Schema {schema_label}"
            )
        else:
            recipe_name = (
                f"GKC Barrel Recipe derived from Wikidata Entity Schema "
                f"{self.validator.eid or 'unknown'}"
            )

        if schema_description:
            recipe_description = (
                f"Mapping schema for validating and building items conforming "
                f"to the specifications in a Wikidata Entity Schema described "
                f"as {schema_description}"
            )
        else:
            recipe_description = (
                "Mapping schema for validating and building items conforming to "
                "the specifications in a Wikidata Entity Schema"
            )

        # Build metadata structure with EntitySchema and classification info
        metadata: dict[str, Any] = {
            "name": recipe_name,
            "description": recipe_description,
            "entity_schema_id": self.validator.eid or "unknown",
            "target_system": "wikidata",
            "generated_date": datetime.now().isoformat(),
        }

        # Add entity type (schema start shape identifier)
        if start_shape_id:
            metadata["entity_type"] = start_shape_id

        # Add entity types (use normalized list format)
        if entity_types:
            if len(entity_types) == 1:
                metadata["target_entity_type"] = entity_types[0]
            else:
                metadata["target_entity_types"] = entity_types

        # Add classification constraints if found in schema
        if instance_of_qids:
            if len(instance_of_qids) == 1:
                metadata["schema_instance_of"] = instance_of_qids[0]
            else:
                metadata["schema_instance_of"] = instance_of_qids

        if subclass_of_qids:
            if len(subclass_of_qids) == 1:
                metadata["schema_subclass_of"] = subclass_of_qids[0]
            else:
                metadata["schema_subclass_of"] = subclass_of_qids

        # Add EntitySchema metadata if available
        if schema_source:
            metadata["source"] = schema_source
        if schema_metadata.get("aliases"):
            metadata["aliases"] = schema_metadata["aliases"]

        mapping = {
            "$schema": "https://example.com/gkc/recipe-schema.json",
            "version": "1.0",
            "metadata": metadata,
        }

        # Add reference library if references are found in schema
        reference_library = self._build_reference_library()
        if reference_library:
            mapping["reference_library"] = reference_library

        # Add qualifier library if qualifiers are found in schema
        qualifier_library = self._build_qualifier_library()
        if qualifier_library:
            mapping["qualifier_library"] = qualifier_library

        # Add mappings
        mapping["mappings"] = {
            "labels": [
                {
                    "source_field": "label",
                    "language": "en",
                    "required": True,
                }
            ],
            "aliases": [
                {
                    "source_field": "aliases",
                    "language": "en",
                    "separator": ";",
                    "required": False,
                }
            ],
            "descriptions": [
                {
                    "source_field": "description",
                    "language": "en",
                    "required": False,
                }
            ],
            "claims": claims_map,
        }

        return mapping

    def generate_gkc_entity_profile(
        self, profile_id: Optional[str] = None
    ) -> dict[str, Any]:
        """
        Generate a minimal GKC Entity Profile from the loaded EntitySchema.

        Creates a canonical entity structure suitable for hand-tuning and storage
        in the SpiritSafe repository. Includes properties extracted from the
        EntitySchema and classification constraints (P31/P279).

        Args:
            profile_id: Optional custom profile ID
                (e.g., 'office-held-by-head-of-government').
                If not provided, will be derived from EntitySchema metadata or EID.

        Returns:
            Dictionary matching GKCEntityProfile schema, ready for JSON serialization

        Raises:
            ValueError: If schema text cannot be loaded
            Exception: If EntitySchema metadata fetch fails
        """
        from gkc.entity_profile import GKCEntityProfile

        if not self.schema_text:
            self.load_specification()

        if not self.schema_text:
            raise ValueError("Schema text could not be loaded")

        # Extract properties and constraints from ShEx
        extractor = SpecificationExtractor(self.schema_text)
        shex_properties = extractor.extract()
        property_ids = sorted(shex_properties.keys())

        # Extract classification constraints (P31 and P279)
        instance_of_qids = extractor.get_instance_of_constraints()
        subclass_of_qids = extractor.get_subclass_of_constraints()

        classification_constraints: dict[str, list[str]] = {}
        if instance_of_qids:
            classification_constraints["p31"] = instance_of_qids
        if subclass_of_qids:
            classification_constraints["p279"] = subclass_of_qids

        # Fetch EntitySchema metadata (labels, descriptions, aliases)
        schema_metadata = {}
        if self.validator.eid:
            try:
                schema_metadata = fetch_entity_schema_metadata(
                    self.validator.eid, user_agent=self.user_agent
                )
            except Exception:
                # Graceful fallback if metadata fetch fails
                schema_metadata = {
                    "label": "",
                    "description": "",
                    "aliases": [],
                }

        # Determine profile ID
        final_profile_id = profile_id
        if not final_profile_id and schema_metadata.get("label"):
            # Derive from EntitySchema label by converting to slug format
            label = schema_metadata["label"]
            final_profile_id = (
                label.lower().replace(" ", "-").replace("/", "-").replace("_", "-")
            )
            # Remove extra dashes
            import re

            final_profile_id = re.sub(r"-+", "-", final_profile_id).strip("-")
        if not final_profile_id and self.validator.eid:
            final_profile_id = f"entity-from-{self.validator.eid.lower()}"
        if not final_profile_id:
            final_profile_id = "unnamed-entity-profile"

        # Build profile dict
        profile_data = {
            "id": final_profile_id,
            "source_eid": self.validator.eid,
            "labels": (
                {_get_preferred_language(): schema_metadata.get("label", "")}
                if schema_metadata.get("label")
                else {}
            ),
            "descriptions": (
                {_get_preferred_language(): schema_metadata.get("description", "")}
                if schema_metadata.get("description")
                else {}
            ),
            "aliases": (
                {_get_preferred_language(): schema_metadata.get("aliases", [])}
                if schema_metadata.get("aliases")
                else {}
            ),
            "properties": property_ids,
            "classification_constraints": classification_constraints,
            "target_systems": ["wikidata"],
        }

        # Validate against model
        profile = GKCEntityProfile(**profile_data)

        # Return as dict for flexibility (JSON serializable, etc.)
        return profile.model_dump()  # type: ignore[return-value]

    @staticmethod
    def _resolve_entity_types(
        explicit_entity_type: Optional[Any],
        instance_of_qids: list[str],
        subclass_of_qids: list[str],
    ) -> list[str]:
        """
        Resolve entity type(s) from explicit parameter or schema constraints.

        Normalizes entity_type into a consistent list format, supporting:
        - Single QID string: 'Q123'
        - List of QIDs: ['Q123', 'Q456']
        - None with schema constraints: uses P31/P279 from schema

        Returns:
            List of QID strings, or empty list if none provided
        """
        if explicit_entity_type is not None:
            # Normalize explicit entity_type to list
            if isinstance(explicit_entity_type, str):
                return [explicit_entity_type]
            elif isinstance(explicit_entity_type, (list, tuple)):
                return list(explicit_entity_type)
            else:
                return []

        # Use schema constraints if no explicit entity_type
        if instance_of_qids:
            return instance_of_qids
        elif subclass_of_qids:
            return subclass_of_qids

        return []

    @staticmethod
    def _format_comment(
        shex_info: dict, prop_data: Optional[PropertyLedgerEntry]
    ) -> str:
        """Format a comment for a property from ShEx and metadata."""
        parts = []
        if shex_info.get("comment"):
            parts.append(shex_info["comment"])
        if prop_data:
            desc = prop_data.get_description(_get_preferred_language())
            if desc:
                parts.append(f"({desc})")
        return " ".join(parts) if parts else "Property mapping"

    @staticmethod
    def _get_transform_hint(datatype: str) -> Optional[dict]:
        """Get transformation hints for common Wikidata datatypes."""
        hints = {
            "time": {"type": "time", "hint": "Provide ISO date or YYYY format"},
            "quantity": {"type": "quantity", "hint": "Numeric value and optional unit"},
            "wikibase-item": {"type": "entity", "hint": "Wikidata QID"},
            "globe-coordinate": {
                "type": "coordinates",
                "hint": "Latitude/longitude pair",
            },
            "monolingualtext": {
                "type": "text",
                "hint": "Text with language tag",
            },
            "url": {"type": "url", "hint": "HTTP or HTTPS URL"},
        }
        return hints.get(datatype)
